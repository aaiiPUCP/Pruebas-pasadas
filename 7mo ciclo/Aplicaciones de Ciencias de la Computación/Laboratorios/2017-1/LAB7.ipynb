{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de BackPropagation\n",
    "\n",
    "En el presente laboratorio Ud. debe diseñar una red neuronal con dos capas intermedias para el problema de reconocimiento de dígitos manuscritos. Para ello debe codear las funciones de:\n",
    "1. Forward propagation (04 puntos)\n",
    "2. Función de costo (02 puntos)\n",
    "3. Back-propagation (08 puntos)\n",
    "4. Predicción (02 puntos)\n",
    "5. Qué parámetros hacen que Ud. encuentre el mejor resultado? (04 puntos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "%matplotlib inline\n",
    "\n",
    "data = loadmat('ex3data1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([10], dtype=uint8),\n",
       " array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0], y_onehot[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20, 401), (30, 21), (10, 31))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial setup\n",
    "input_size = 400\n",
    "hidden_size1 = 20\n",
    "hidden_size2 = 30\n",
    "num_labels = 10\n",
    "learning_rate = 1\n",
    "\n",
    "# randomly initialize a parameter array of the size of the full network's parameters\n",
    "params = (np.random.random(size=hidden_size1 * (input_size + 1) + hidden_size2 * (hidden_size1 + 1) + num_labels * (hidden_size2 + 1)) - 0.5) * 0.25\n",
    "\n",
    "m = X.shape[0]\n",
    "X = np.matrix(X)\n",
    "y = np.matrix(y)\n",
    "\n",
    "# unravel the parameter array into parameter matrices for each layer\n",
    "theta1 = np.matrix(np.reshape(params[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
    "theta2 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1):hidden_size1*(input_size+1)+hidden_size2*(hidden_size1+1)], (hidden_size2, (hidden_size1 + 1))))\n",
    "theta3 = np.matrix(np.reshape(params[hidden_size1*(input_size+1)+hidden_size2*(hidden_size1+1):], (num_labels, (hidden_size2 + 1))))\n",
    "\n",
    "theta1.shape, theta2.shape, theta3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagate(X, theta1, theta2, theta3):\n",
    "    #M=5000\n",
    "    m = X.shape[0]\n",
    "    #Insert a 1 at the start of the initial variables (biased number)\n",
    "    a1 = np.insert(X, 0, values=np.ones(m), axis=1)\n",
    "    #Generate Activation values that takes us from layer 1 to layer 2, save them in z2\n",
    "    z2 = a1 * theta1.T\n",
    "    #Apply sigmoid function to every activation value (Z2), and\n",
    "    #Insert a 1 at the start of these activation values (biased number)\n",
    "    a2 = np.insert(sigmoid(z2), 0, values=np.ones(m), axis=1)\n",
    "    #Generate Activation value that takes us from layer 2 to layer 3, save them in z3\n",
    "    z3 = a2 * theta2.T\n",
    "    #Apply sigmoid function to every activation value (Z3), and\n",
    "    #Insert a 1 at the start of these activation values (biased number)\n",
    "    a3 = np.insert(sigmoid(z3), 0, values=np.ones(m), axis=1)\n",
    "    #Generate Activation value that takes us from layer 3 to layer 4 (final one), and save them in z4\n",
    "    z4 = a3 * theta3.T\n",
    "    #Apply Sigmoid Function to z4, and that's our answer\n",
    "    h = sigmoid(z4)\n",
    "    return a1, z2, a2, z3, a3, z4, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 401),\n",
       " (5000, 20),\n",
       " (5000, 21),\n",
       " (5000, 30),\n",
       " (5000, 31),\n",
       " (5000, 10),\n",
       " (5000, 10))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A1 = Layer 1 (initial) but with the biased number at the start\n",
    "#Z2 = Activation values at layer 2\n",
    "#A2 = Activation values at layer 2 but with a sigmoid applied to them, plus a biased number at the start\n",
    "#Z3 = Activation values at layer 3\n",
    "#A3 = Activation values at layer 3 but with a sigmoid applied to them, plus a biased number at the start\n",
    "#Z4 = Activation value at layer 4 (final one)\n",
    "#H = Final result\n",
    "a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
    "a1.shape, z2.shape, a2.shape, z3.shape, a3.shape, z4.shape, h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(params, input_size, hidden_size1, hidden_size2, num_labels, X, y, learning_rate):\n",
    "    #M = 5000\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # unravel the parameter array into parameter matrices for each layer\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1):hidden_size1*(input_size+1)+hidden_size2*(hidden_size1+1)], (hidden_size2, (hidden_size1 + 1))))\n",
    "    theta3 = np.matrix(np.reshape(params[hidden_size1*(input_size+1)+hidden_size2*(hidden_size1+1):], (num_labels, (hidden_size2 + 1))))\n",
    "\n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
    "    \n",
    "    # compute the cost\n",
    "    J = 0\n",
    "    # Using the formula, we iterate through every example\n",
    "    for i in range(m):\n",
    "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
    "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
    "        J += np.sum(first_term - second_term)\n",
    "    \n",
    "    J = J / m\n",
    "    \n",
    "    # add the cost regularization term\n",
    "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)) + np.sum(np.power(theta3[:,1:], 2)))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.7926084664272075"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(params, input_size, hidden_size1, hidden_size2, num_labels, X, y_onehot, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    return np.multiply(sigmoid(z), (1 - sigmoid(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backprop(params, input_size, hidden_size1, hidden_size2, num_labels, X, y, learning_rate):\n",
    "    m = X.shape[0]\n",
    "    X = np.matrix(X)\n",
    "    y = np.matrix(y)\n",
    "    \n",
    "    # unravel the parameter array into parameter matrices for each layer\n",
    "    theta1 = np.matrix(np.reshape(params[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(params[hidden_size1 * (input_size + 1):hidden_size1*(input_size+1)+hidden_size2*(hidden_size1+1)], (hidden_size2, (hidden_size1 + 1))))\n",
    "    theta3 = np.matrix(np.reshape(params[hidden_size1*(input_size+1)+hidden_size2*(hidden_size1+1):], (num_labels, (hidden_size2 + 1))))\n",
    "\n",
    "    # run the feed-forward pass\n",
    "    a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
    "    \n",
    "    # initializations\n",
    "    J = 0\n",
    "    delta1 = np.zeros(theta1.shape)\n",
    "    delta2 = np.zeros(theta2.shape)\n",
    "    delta3 = np.zeros(theta3.shape)\n",
    "    \n",
    "    # Using the formula, we iterate through every example\n",
    "    for i in range(m):\n",
    "        first_term = np.multiply(-y[i,:], np.log(h[i,:]))\n",
    "        second_term = np.multiply((1 - y[i,:]), np.log(1 - h[i,:]))\n",
    "        J += np.sum(first_term - second_term)\n",
    "    \n",
    "    J = J / m\n",
    "    \n",
    "    # add the cost regularization term\n",
    "    J += (float(learning_rate) / (2 * m)) * (np.sum(np.power(theta1[:,1:], 2)) + np.sum(np.power(theta2[:,1:], 2)) + np.sum(np.power(theta3[:,1:], 2)))\n",
    "    \n",
    "    \n",
    "    #With cost already calculated,\n",
    "    #perform backpropagation\n",
    "    for t in range(m):\n",
    "        #For every example\n",
    "        #A1T = Data of example t (layer 1)\n",
    "        a1t = a1[t,:]\n",
    "        #Z2T = Activation values at layer 2 of example t\n",
    "        z2t = z2[t,:]  \n",
    "        #A2T = Sigmoided Z2T of example t\n",
    "        a2t = a2[t,:]  \n",
    "        #Z3T = Activation values at layer 3 of example t\n",
    "        z3t = z3[t,:]\n",
    "        #A3T = Activation values at layer 3 but with a sigmoid applied to them, plus a biased number at the start\n",
    "        a3t = a3[t,:]\n",
    "        #Z4T = Activation value at layer 4 (final one)\n",
    "        z4t = z4[t,:]\n",
    "        #HT = Final Result of example t\n",
    "        ht = h[t,:]  # (1, 10)\n",
    "        #Y = Actual Result of example t\n",
    "        yt = y[t,:]  # (1, 10)\n",
    "        #D4T = Error at example t\n",
    "        d4t = ht - yt  # (1, 10)\n",
    "        \n",
    "        #Insert Ones in Z3T\n",
    "        z3t = np.insert(z3t, 0, values=np.ones(1))\n",
    "        #Multiply elements of D4T by theta3 and by the sigmoided Z3T, you get D3T\n",
    "        d3t = np.multiply((theta3.T * d4t.T).T, sigmoid_gradient(z3t))\n",
    "        \n",
    "        #Insert Ones in Z2T\n",
    "        z2t = np.insert(z2t, 0, values=np.ones(1))\n",
    "        #Multiply elements of D3T by theta2 and by the sigmoided Z2T, you get D2T\n",
    "        d2t = np.multiply((theta2.T * d3t[:,1:].T).T, sigmoid_gradient(z2t))\n",
    "        #That \n",
    "        delta1 = delta1 + (d2t[:,1:]).T * a1t\n",
    "        delta2 = delta2 + (d3t[:,1:]).T * a2t\n",
    "        delta3 = delta3 + d4t.T * a3t\n",
    "        \n",
    "    delta1 = delta1 / m\n",
    "    delta2 = delta2 / m\n",
    "    delta3 = delta3 / m\n",
    "    \n",
    "    # add the gradient regularization term\n",
    "    delta1[:,1:] = delta1[:,1:] + (theta1[:,1:] * learning_rate) / m\n",
    "    delta2[:,1:] = delta2[:,1:] + (theta2[:,1:] * learning_rate) / m\n",
    "    delta3[:,1:] = delta3[:,1:] + (theta3[:,1:] * learning_rate) / m\n",
    "    \n",
    "    # unravel the gradient matrices into a single array\n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2), np.ravel(delta3)))\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.7926084664272075, (8960,))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J, grad = backprop(params, input_size, hidden_size1, hidden_size2, num_labels, X, y_onehot, learning_rate)\n",
    "J, grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.19188104219025026\n",
       "     jac: array([ -5.05260256e-04,  -2.73150268e-06,   9.01410762e-07, ...,\n",
       "         1.35620341e-04,   1.32223448e-04,   2.29977941e-04])\n",
       " message: 'Max. number of function evaluations reached'\n",
       "    nfev: 250\n",
       "     nit: 25\n",
       "  status: 3\n",
       " success: False\n",
       "       x: array([ 0.79726096, -0.02731503,  0.00901411, ..., -0.51868121,\n",
       "        0.3354375 ,  0.19514874])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "learning_rate = 0.5\n",
    "# minimize the objective function\n",
    "fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size1,hidden_size2, num_labels, X, y_onehot, learning_rate), \n",
    "                method='TNC', jac=True, options={'maxiter': 250})\n",
    "fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.29944840e-07   6.94214124e-03   7.68360181e-07   4.70169943e-10\n",
      "    4.17845484e-05   7.72595901e-04   3.33022216e-05   5.01719598e-04\n",
      "    6.78750055e-05   9.98905833e-01]]\n"
     ]
    }
   ],
   "source": [
    "X = np.matrix(X)\n",
    "# unravel the parameter array into parameter matrices for each layer\n",
    "theta1 = np.matrix(np.reshape(fmin.x[:hidden_size1 * (input_size + 1)], (hidden_size1, (input_size + 1))))\n",
    "theta2 = np.matrix(np.reshape(fmin.x[hidden_size1 * (input_size + 1):hidden_size1*(input_size+1)+hidden_size2*(hidden_size1+1)], (hidden_size2, (hidden_size1 + 1))))\n",
    "theta3 = np.matrix(np.reshape(fmin.x[hidden_size1*(input_size+1)+hidden_size2*(hidden_size1+1):], (num_labels, (hidden_size2 + 1))))\n",
    "\n",
    "a1, z2, a2, z3, a3, z4, h = forward_propagate(X, theta1, theta2, theta3)\n",
    "y_pred = np.array(np.argmax(h, axis=1) + 1)\n",
    "print h[125]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 99.84%\n"
     ]
    }
   ],
   "source": [
    "correct = [1 if a == b else 0 for (a, b) in zip(y_pred, y)]\n",
    "accuracy = (sum(map(int, correct)) / float(len(correct)))\n",
    "print 'accuracy = {0}%'.format(accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############################################################\n",
    "#      Que parametros generan los mejores resultados?         #\n",
    "###############################################################\n",
    "#  Learning Rate      Capa1       Capa2      Accuracy\n",
    "#     1               20          30           99.1\n",
    "#     0.8             20          30           99.72\n",
    "#     0.5             20          30           99.84\n",
    "\n",
    "#Investigacion:\n",
    "#Dada una tasa de aprendizaje alta, como lo es 0.5, utilizar varios elementos en cada capa resulta ser muy ineficiente.[1]\n",
    "#De esta manera, concluimos que el numero de unidades escondidas (50), resulta más efectivo que números elevados como 600,\n",
    "#aún sin haberlo probado en nuestro modelo (lo cual tardaría bastante tiempo, además).\n",
    "\n",
    "#[1] David Stutz, Pavel Golik, Ralf Schlüter, and Hermann Ney. Introduction to Neural Networks. \n",
    "#Seminar on Selected Topics in Human Language Technology and Pattern Recognition, 2014.\n",
    "#Encontrado en: http://davidstutz.de/seminar-paper-introduction-neural-networks/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
