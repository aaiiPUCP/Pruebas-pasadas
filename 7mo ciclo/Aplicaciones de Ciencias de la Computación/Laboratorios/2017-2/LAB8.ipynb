{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el presente laboratorio se abordarán técnicas de Procesamiento de Lenguaje Natural con el fin de crear modelos de clasificación de texto. Para ello se utilizará un dataset de noticias extraídas de diversas páginas web entre Marzo y Agosto del 2014. Las categorías de noticias incluyen negocios, ciencia y tecnología, entretenimiento y salud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/machine-learning-databases/00359/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contenido\n",
    "Del repositorio del dataset se tiene la siguiente información sobre sus atributos:\n",
    "- **ID** : the numeric ID of the article\n",
    "- **TITLE** : the headline of the article\n",
    "- **URL** : the URL of the article\n",
    "- **PUBLISHER** : the publisher of the article\n",
    "- **CATEGORY** : the category of the news item; one of: -- b : business -- t : science and technology -- e : entertainment -- m : health\n",
    "- **STORY** : alphanumeric ID of the news story that the article discusses\n",
    "- **HOSTNAME** : hostname where the article was posted\n",
    "- **TIMESTAMP** : approximate timestamp of the article's publication, given in Unix time (seconds since midnight on Jan 1, 1970)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para el desarrollo del laboratorio se puede hacer uso del módulo stopwords de la liberia nltk, para ello se puede utilizar el siguiente código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Brenda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Brenda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para realizar la lematización, se requiere instalar la base de datos de WordNet, para lo cual puede utilizar el siguiente comando\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Además, se puede consultar en el siguiente link para las tareas de Lemmatization, Stemming y expresiones regulares http://www.nltk.org/book/ch03.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploración (1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Descargue y cargue el dataset. Identifique la cantidad de noticias por cada categoría (1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de noticias por cada categoria: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'b': 115967, 'e': 152469, 'm': 45639, 't': 108344})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "train_data=pd.read_csv('newsCorpora.csv',sep='\\t')\n",
    "categorias = np.asarray(train_data['category'])\n",
    "counterCategorias = Counter(categorias)\n",
    "print(\"Cantidad de noticias por cada categoria: \")\n",
    "counterCategorias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento (9p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Considere solo los títulos de los artículos para clasificar las noticias. Remueva las otras columnas del dataset (1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=np.asarray(train_data['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b1. Explore los títulos de los artículos del dataset y describa qué caracteres de puntuación (u otros caracteres) considera que deben ser removidos (0.5p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fed's Charles Plosser sees high bar for change in pace of tapering\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b2. Elimine dichos caracteres del dataset utilizando expresiones regulares (1.5p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '-']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "puntuacion=[',','-']\n",
    "print(puntuacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_preprocessed = []\n",
    "doc_tokens = []\n",
    "\n",
    "for d in train_data:\n",
    "    no_puntuacion = d.translate(puntuacion)\n",
    "    doc_preprocessed.append(no_puntuacion)\n",
    "    t = nltk.word_tokenize(no_puntuacion)\n",
    "    doc_tokens.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Fed's Charles Plosser sees high bar for change in pace of tapering\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_preprocessed[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Convierta el texto del dataset a minúsculas (0.5p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"fed's charles plosser sees high bar for change in pace of tapering\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(doc_preprocessed)):\n",
    "    doc_preprocessed[i]=doc_preprocessed[i].lower()\n",
    "doc_preprocessed[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Remueva stopwords del texto del dataset e indique la cantidad de stopwords que está removiendo (1.5p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "palabras=stopwords.words('english')\n",
    "\n",
    "for linea in doc_preprocessed:\n",
    "    tokens = word_tokenize(linea)\n",
    "    filtered_sentence = []\n",
    "    for i in tokens:\n",
    "        if i not in palabras:\n",
    "            filtered_sentence.append(i)\n",
    "    linea=filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us open: stocks fall after fed official hints at accelerated tapering'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_preprocessed[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e. Genere dos versiones o copias del dataset (*data_stem* y *data_lem*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stem=doc_preprocessed\n",
    "data_lem=doc_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### e1. Sobre la primera copia (data_stem), aplique Stemming (2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "for linea in data_stem:\n",
    "    word=word_tokenize(linea)\n",
    "    aux=\"\"\n",
    "    for palabra in word:\n",
    "        pal=stemmer.stem(palabra)\n",
    "        aux=aux+pal+\" \"\n",
    "    linea=aux\n",
    "    #print(linea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### e2. Sobre la segunda copia (data_lem), aplique Lemmatization (2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem=WordNetLemmatizer()\n",
    "for linea in data_lem:\n",
    "    word = word_tokenize(linea)\n",
    "    auxL=\"\"\n",
    "    for palabra in word:\n",
    "        pal=lem.lemmatize(palabra)\n",
    "        aux=aux+pal+\" \"\n",
    "    linea=aux\n",
    "    #print(linea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vectorización del texto (4p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Para cada copia del dataset (*data_stem* y *data_lem*) convierta el texto del título de las noticias a vectores de características utlizando CountVectorizer y TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Las clases CountVectorizer y TfidfVectorizer se encuentran en la librería scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a1. Vectorización de data_stem utilizando CountVectorizer (X1) (1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422419, 54637)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "X1=cv.fit_transform(data_stem)\n",
    "X1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a2. Vectorización de data_stem utilizando TfidfVectorizer (X2) (1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422419, 54637)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X2 = tfidf.fit_transform(data_stem)\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a3. Vectorización de data_lem utilizando CountVectorizer (X3) (1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422419, 54637)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "X3=cv.fit_transform(data_lem)\n",
    "X3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a4. Vectorización de data_lem utilizando TfidfVectorizer (X4) (1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(422419, 54637)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X4 = tfidf.fit_transform(data_lem)\n",
    "X4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento de modelos de aprendizaje de máquina (7p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Realice un train test split de cada dataset (X1,X2, X3, X4), considerando un tamaño de conjunto de prueba de 20% (1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asegúrese de realizar la misma partición para cada dataset, para que los resultados sean comparables (i.e. si utiliza la función *train_test_split()* fije el parámetro *random_state* con el mismo valor para X1, X2, X3 y X4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "train_data=pd.read_csv('newsCorpora.csv',sep='\\t')\n",
    "category = np.asarray(train_data['category'])\n",
    "\n",
    "X1_train,X1_test,y1_train,y1_test=train_test_split(X1, category, test_size=0.2, random_state=42)\n",
    "X2_train,X2_test,y2_train,y2_test=train_test_split(X2, category, test_size=0.2, random_state=42)\n",
    "X3_train,X3_test,y3_train,y3_test=train_test_split(X3, category, test_size=0.2, random_state=42)\n",
    "X4_train,X4_test,y4_train,y4_test=train_test_split(X4, category, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Entrene por lo menos tres modelos de aprendizaje de máquina con cada conjunto de entrenamiento (X1_train, X2_train, X3_train, X4_train) utilizando validación cruzada con 5 iteraciones (folds) (3p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Si el tiempo de entrenamiento del modelo es demasiado alto, considere generar vectores de características más pequeños modificando el parámetro __max features__ tanto de CountVectorizer como de TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def run_model(clf, X, y):\n",
    "    scores = cross_val_score(clf, X, y, cv=4)\n",
    "    print(\"%s accuracy: %0.2f (+/- %0.2f)\" % \\\n",
    "          (str(clf.__class__).split('.')[-1].replace('>','').replace(\"'\",''), \n",
    "          scores.mean(), scores.std() * 2))\n",
    "    \n",
    "def run_models(X, y):\n",
    "    run_model(PassiveAggressiveClassifier(), X, y)\n",
    "    run_model(SGDClassifier(), X, y)\n",
    "    run_model(MultinomialNB(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Presente una tabla con los 12 resultados de entrenamiento de cada modelo (3) con cada dataset (4) con los siguientes campos (1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Modelo utilizado\n",
    "- Dataset utlizado\n",
    "- Media de accuracy de las iteraciones de la validación cruzada\n",
    "- Desviación estándar de las iteraciones de la validación cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Con X1: \n",
      "PassiveAggressiveClassifier accuracy: 0.94 (+/- 0.00)\n",
      "SGDClassifier accuracy: 0.94 (+/- 0.00)\n",
      "MultinomialNB accuracy: 0.93 (+/- 0.00)\n",
      "\n",
      "Con X2: \n",
      "PassiveAggressiveClassifier accuracy: 0.95 (+/- 0.00)\n",
      "SGDClassifier accuracy: 0.92 (+/- 0.00)\n",
      "MultinomialNB accuracy: 0.92 (+/- 0.00)\n",
      "\n",
      "Con X3: \n",
      "PassiveAggressiveClassifier accuracy: 0.94 (+/- 0.00)\n",
      "SGDClassifier accuracy: 0.94 (+/- 0.00)\n",
      "MultinomialNB accuracy: 0.93 (+/- 0.00)\n",
      "\n",
      "Con X4: \n",
      "PassiveAggressiveClassifier accuracy: 0.94 (+/- 0.00)\n",
      "SGDClassifier accuracy: 0.92 (+/- 0.00)\n",
      "MultinomialNB accuracy: 0.92 (+/- 0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Con X1: \")\n",
    "run_models(X1_train,y1_train)\n",
    "print()\n",
    "\n",
    "print(\"Con X2: \")\n",
    "run_models(X2_train,y2_train)\n",
    "print()\n",
    "\n",
    "print(\"Con X3: \")\n",
    "run_models(X3_train,y3_train)\n",
    "print()\n",
    "\n",
    "print(\"Con X4: \")\n",
    "run_models(X4_train,y4_train)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Seleccione los 3 mejores resultados y para cada uno, realice la predicción sobre el conjunto de prueba respectivo (X1_test, X2_test, X3_test o X4_test según corresponda), y reporte el accuracy obtenido y la matriz de confusión de las predicciones (2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para X1_test:\n",
      "Train accuracy 96.87041333270206%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          b       0.95      0.96      0.96     23414\n",
      "          e       0.98      0.99      0.99     30353\n",
      "          m       0.98      0.96      0.97      9024\n",
      "          t       0.96      0.96      0.96     21693\n",
      "\n",
      "avg / total       0.97      0.97      0.97     84484\n",
      "\n",
      "-----------------------------------------------------\n",
      "Para X2_test:\n",
      "Train accuracy 94.56465129491974%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          b       0.93      0.93      0.93     23414\n",
      "          e       0.96      0.98      0.97     30353\n",
      "          m       0.97      0.89      0.93      9024\n",
      "          t       0.93      0.93      0.93     21693\n",
      "\n",
      "avg / total       0.95      0.95      0.95     84484\n",
      "\n",
      "-----------------------------------------------------\n",
      "Para X3_test:\n",
      "Train accuracy 96.87041333270206%\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          b       0.95      0.96      0.96     23414\n",
      "          e       0.98      0.99      0.99     30353\n",
      "          m       0.98      0.96      0.97      9024\n",
      "          t       0.96      0.96      0.96     21693\n",
      "\n",
      "avg / total       0.97      0.97      0.97     84484\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model1 = LogisticRegression()\n",
    "model1.fit(X1_test, y1_test)\n",
    "p1 = model1.predict(X1_test)\n",
    "print(\"Para X1_test:\")\n",
    "print('Train accuracy {}%'.format(100*np.sum(p1.T == y1_test.ravel())/p1.T.size))\n",
    "print(classification_report(y1_test, p1))\n",
    "print(\"----------------------------------------------------------------------\")\n",
    "\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(X2_test, y2_test)\n",
    "p2 = model2.predict(X2_test)\n",
    "print(\"Para X2_test:\")\n",
    "print('Train accuracy {}%'.format(100*np.sum(p2.T == y2_test.ravel())/p2.T.size))\n",
    "print(classification_report(y2_test, p2))\n",
    "print(\"-----------------------------------------------------------------------\")\n",
    "\n",
    "model3 = LogisticRegression()\n",
    "model3.fit(X3_test, y3_test)\n",
    "p3 = model3.predict(X3_test)\n",
    "print(\"Para X3_test:\")\n",
    "print('Train accuracy {}%'.format(100*np.sum(p3.T == y3_test.ravel())/p3.T.size))\n",
    "print(classification_report(y3_test, p3))\n",
    "print(\"-----------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
